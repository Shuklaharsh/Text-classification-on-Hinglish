{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC5W-exzcN3v",
        "outputId": "e2b61500-4816-4416-e2af-4186437f944d"
      },
      "source": [
        "  !pip install fasttext\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 19.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 11.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3097940 sha256=51db36769b19d605ac38b9794f5163d9ddca579a3446fb6001874fc13f49f359\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0-bCCmEaGN1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb6c4f7e-d301-4363-c20e-05c3f1fffdc2"
      },
      "source": [
        "import numpy as np\n",
        "#import fasttext\n",
        "#from fasttext import FastVector\n",
        "import pandas as pd\n",
        "import keras  \n",
        "from keras.layers import concatenate,merge,add\n",
        " \n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D\n",
        "from keras import regularizers\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import re\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
        "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from gensim.models.wrappers import FastText\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import keras\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/trainv2.csv',header=0)\n",
        "#df_dev = pd.read_csv('DataPre-Processing/dev.tsv',sep = '\\t', header=0)\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/testv2.csv',header=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_train.head\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_X = [str(x).lower() for x in df_train['text']]\n",
        "#dev_X= [str(x).lower() for x in df_train['text']]\n",
        "test_X = [str(x).lower()  for x in df_test['text']]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_Y = [str(x) for x in df_train['tag']]\n",
        "#ved_Y = [str(x) for x in df_ved['tag']]\n",
        "test_Y = [str(x) for x in df_test['tag']]\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "print('here 1')\n",
        "df_train['tag'].value_counts().plot(kind='bar')\n",
        "print(df_train['tag'].value_counts())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_test['tag'].value_counts().plot(kind='bar')\n",
        "print(df_test['tag'].value_counts())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_words=35000          \n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words,char_level=False, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "                                   lower=False,split=' ')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tokenizer.fit_on_texts(train_X)\n",
        "\n",
        "tokenizer.fit_on_texts(test_X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "word_index=tokenizer.word_index\n",
        "\n",
        "\n",
        "\n",
        "X = tokenizer.texts_to_sequences(train_X)\n",
        "\n",
        "test_X = tokenizer.texts_to_sequences(test_X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"here 2\")\n",
        "print(len(X))\n",
        "print(len(test_X))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X=pad_sequences(X, maxlen= 50)                                     \n",
        "test_X=pad_sequences(test_X, maxlen= 50)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(test_X.shape)\n",
        "test_X.shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Y = pd.get_dummies(train_Y)\n",
        "\n",
        "Y_test = pd.get_dummies(test_Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(Y_test)\n",
        "\n",
        "print(Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import gensim\n",
        "import numpy\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "sg_w2v_model=KeyedVectors.load_word2vec_format('/content/drive/MyDrive/modelcbow.bin')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import fasttext\n",
        "\n",
        "\n",
        "sg_w2v_modelC = fasttext.load_model('/content/drive/MyDrive/fasttextagg_unsuperv.bin')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "len(word_index)\n",
        "\n",
        "\n",
        "\n",
        "q=0\n",
        "EMBEDDING_DIM=100\n",
        "NUM_WORDS=34167\n",
        "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Fasttext\n",
        "q=0\n",
        "# embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "# for word, i in word_index.items():\n",
        "#     if i>=NUM_WORDS:\n",
        "#         continue\n",
        "#     try:\n",
        "#       if word in sg_w2v_model:\n",
        "#         embedding_vector = sg_w2v_model[word]\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "#       else:\n",
        "#         embedding_vector = sg_w2v_modelC[word]\n",
        "#         embedding_matrix[i] = embedding_vector\n",
        "#     except KeyError:\n",
        "#         #embedding_matrix[i]=0\n",
        "#         embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "#         q=q+1\n",
        "    \n",
        "\n",
        "# from keras.layers import Embedding\n",
        "# embedding_layer = Embedding(vocabulary_size,\n",
        "#                             EMBEDDING_DIM,\n",
        "#                             weights=[embedding_matrix],\n",
        "#                             trainable=False)\n",
        "\n",
        "# embedding_matrix.shape\n",
        "\n",
        "\n",
        "# In[25]:\n",
        "\n",
        "\n",
        "# Word2vec....\n",
        "r=0\n",
        "\n",
        "embedding_matrix2 = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i>=NUM_WORDS:\n",
        "        continue\n",
        "    try:\n",
        "        \n",
        "        embedding_vector2 = sg_w2v_modelC[word]\n",
        "        embedding_matrix2[i] = embedding_vector2\n",
        "    except KeyError:\n",
        "        embedding_matrix2[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
        "        r=r+1\n",
        "print(r)        \n",
        "\n",
        "\n",
        "\n",
        "filter_length = 3\n",
        "nb_filter = 128\n",
        "pool_length = 3\n",
        "# LSTM\n",
        "\n",
        "\n",
        "# In[28]:\n",
        "\n",
        "\n",
        "from keras.layers import TimeDistributed,SpatialDropout1D\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import LSTM, BatchNormalization, Bidirectional\n",
        "embed_dim=100;\n",
        "model_blstm = Sequential()\n",
        "model_blstm.add(Embedding(vocabulary_size, embed_dim,\n",
        "          weights=[embedding_matrix2], input_length=50, trainable=False))\n",
        "\n",
        "model_blstm.add( SpatialDropout1D(0.2))\n",
        "\n",
        "model_blstm.add(Bidirectional(LSTM(units=80)))\n",
        "model_blstm.add(Dropout(0.2))\n",
        "\n",
        "model_blstm.add((Dense(32, activation='softmax')))\n",
        "model_blstm.add(Dropout(0.2))\n",
        "model_blstm.add((Dense(3, activation='softmax')))\n",
        "\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "model_blstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_blstm.summary()\n",
        "\n",
        "\n",
        "# In[32]:\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "    #Trains model for 50 epochs with shuffling after every epoch for training data and validates on validation data\n",
        "print('Train...')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
        "history=model_blstm.fit(X, Y, batch_size=32, epochs=20,\n",
        "         validation_split=0.2,callbacks=[early_stopping] )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "score, acc = model_blstm.evaluate(test_X, Y_test, batch_size=32)\n",
        "print('bilstm')\n",
        "print(score, acc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.layers import Embedding,Dropout,SpatialDropout1D,GlobalMaxPool1D\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras import initializers\n",
        "from keras.optimizers import Adam\n",
        "class AttLayer(Layer):\n",
        "    def init(self, **kwargs):\n",
        "        self.init = initializers.get('normal')\n",
        "        super(AttLayer, self).init(**kwargs)\n",
        "        \n",
        "          \n",
        "\n",
        "    def build(self,input_shape):\n",
        "        print(input_shape)\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\", trainable = True)\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(120,),initializer=\"normal\", trainable = True)    \n",
        "        super(AttLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        print(\"call\",x,self.W)\n",
        "        eij = K.tanh(K.dot(x, self.W)+self.b)\n",
        "       \n",
        "        a = K.softmax(eij, axis=1)\n",
        "        output = x*a\n",
        "        c=(K.sum(output,axis=1))\n",
        "        print(\"c shape\",c[3])\n",
        "        return (K.sum(output,axis=1))\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \n",
        "            return input_shape[0], input_shape[-1]\n",
        "        \n",
        "\n",
        "        \n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix2],\n",
        "                            input_length=50,\n",
        "                            trainable=True)\n",
        "\n",
        "sequence_input = Input(shape=(50,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n",
        "\n",
        "lstm = Bidirectional(LSTM(60, return_sequences=True))(embedded_sequences)\n",
        "\n",
        "\n",
        "l_att = AttLayer( )(lstm)\n",
        "print(l_att.shape)\n",
        "\n",
        "\n",
        "d = Dropout(0.2)(l_att)\n",
        "x = Dense(32, activation='relu')(d)\n",
        "\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "preds = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(sequence_input, preds)\n",
        "\n",
        "#print((features))\n",
        "adam =Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# In[55]:\n",
        "\n",
        "\n",
        "print('Train...')\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
        "history=model.fit(X, Y, batch_size=32, epochs=6, validation_split=0.33,shuffle=True,callbacks=[early_stopping] )\n",
        "\n",
        "\n",
        "# In[56]:\n",
        "\n",
        "\n",
        "pred = model.predict(test_X, batch_size = 32, verbose = 1)\n",
        "print(pred.shape)\n",
        "score, acc = model.evaluate(test_X, Y_test, batch_size=32)\n",
        "print('here')\n",
        "print(score, acc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#bilstm \n",
        "predict = model_blstm.predict(test_X)\n",
        "test_y = Y_test.values\n",
        "\n",
        "tmpMat = np.zeros((len(Y_test), 3), dtype=int)\n",
        "\n",
        "for i in range(0,len(predict)):\n",
        "    if(predict[i][0] > predict[i][1] and predict[i][0] > predict[i][2]):\n",
        "        tmpMat[i][0]=1\n",
        "    elif(predict[i][1] > predict[i][0] and predict[i][1] > predict[i][2]):\n",
        "        tmpMat[i][1]=1\n",
        "    else:\n",
        "        tmpMat[i][2]=1\n",
        "print(accuracy_score(test_y,tmpMat))\n",
        "print(precision_score(test_y, tmpMat, average='macro'))\n",
        "print(recall_score(test_y, tmpMat, average='macro'))\n",
        "print('f1 bilstm')\n",
        "print(f1_score(test_y, tmpMat, average='macro'))\n",
        "\n",
        "print(len(tmpMat))\n",
        "\n",
        "\n",
        "\n",
        "# Bilstm attention .\n",
        "predict = model.predict(test_X)                 \n",
        "test_y = Y_test.values\n",
        "\n",
        "tmpMat = np.zeros((len(Y_test), 3), dtype=int)\n",
        "\n",
        "for i in range(0,len(predict)):\n",
        "    if(predict[i][0] > predict[i][1] and predict[i][0] > predict[i][2]):\n",
        "        tmpMat[i][0]=1\n",
        "    elif(predict[i][1] > predict[i][0] and predict[i][1] > predict[i][2]):\n",
        "        tmpMat[i][1]=1\n",
        "    else:\n",
        "        tmpMat[i][2]=1\n",
        "print(accuracy_score(test_y,tmpMat))\n",
        "print(precision_score(test_y, tmpMat, average='weighted'))\n",
        "print(recall_score(test_y, tmpMat, average='weighted'))\n",
        "print('f1 bilstm attention')\n",
        "print(f1_score(test_y, tmpMat, average='weighted'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Of class CAG\n",
        "print('CAG')\n",
        "print(accuracy_score(test_y[:,0],tmpMat[:,0]))\n",
        "print(precision_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n",
        "print(recall_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n",
        "print(f1_score(test_y[:,0], tmpMat[:,0], average='weighted'))\n",
        "\n",
        "\n",
        "# Of class NAG\n",
        "print('NAG')\n",
        "print(accuracy_score(test_y[:,1],tmpMat[:,1]))\n",
        "print(precision_score(test_y[:,1], tmpMat[:,1], average='macro'))\n",
        "print(recall_score(test_y[:,1], tmpMat[:,1], average='macro'))\n",
        "print(f1_score(test_y[:,1], tmpMat[:,1], average='macro'))\n",
        "\n",
        "# Of class OAG\n",
        "print('OAG')\n",
        "print(accuracy_score(test_y[:,2],tmpMat[:,2]))\n",
        "print(precision_score(test_y[:,2], tmpMat[:,2], average='macro'))\n",
        "print(recall_score(test_y[:,2], tmpMat[:,2], average='macro'))\n",
        "print(f1_score(test_y[:,2], tmpMat[:,2], average='macro'))\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here 1\n",
            "NAG    1224\n",
            "CAG     855\n",
            "OAG     688\n",
            "Name: tag, dtype: int64\n",
            "NAG    445\n",
            "CAG    176\n",
            "OAG     71\n",
            "Name: tag, dtype: int64\n",
            "here 2\n",
            "2767\n",
            "692\n",
            "(692, 50)\n",
            "     CAG  NAG  OAG\n",
            "0      0    1    0\n",
            "1      0    0    1\n",
            "2      0    1    0\n",
            "3      1    0    0\n",
            "4      1    0    0\n",
            "..   ...  ...  ...\n",
            "687    0    1    0\n",
            "688    0    1    0\n",
            "689    0    0    1\n",
            "690    0    0    1\n",
            "691    0    1    0\n",
            "\n",
            "[692 rows x 3 columns]\n",
            "      CAG  NAG  OAG\n",
            "0       1    0    0\n",
            "1       0    1    0\n",
            "2       0    1    0\n",
            "3       1    0    0\n",
            "4       0    0    1\n",
            "...   ...  ...  ...\n",
            "2762    0    0    1\n",
            "2763    1    0    0\n",
            "2764    0    1    0\n",
            "2765    0    0    1\n",
            "2766    0    0    1\n",
            "\n",
            "[2767 rows x 3 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 100)           968900    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_2 (Spatial (None, 50, 100)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 160)               115840    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                5152      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 1,089,991\n",
            "Trainable params: 121,091\n",
            "Non-trainable params: 968,900\n",
            "_________________________________________________________________\n",
            "Train...\n",
            "Epoch 1/20\n",
            "70/70 [==============================] - 10s 81ms/step - loss: 1.0908 - accuracy: 0.3628 - val_loss: 1.0533 - val_accuracy: 0.4838\n",
            "Epoch 2/20\n",
            "70/70 [==============================] - 4s 63ms/step - loss: 1.0730 - accuracy: 0.4364 - val_loss: 1.0438 - val_accuracy: 0.4838\n",
            "Epoch 3/20\n",
            "70/70 [==============================] - 4s 63ms/step - loss: 1.0677 - accuracy: 0.4398 - val_loss: 1.0056 - val_accuracy: 0.5433\n",
            "Epoch 4/20\n",
            "70/70 [==============================] - 4s 63ms/step - loss: 1.0140 - accuracy: 0.5203 - val_loss: 0.9425 - val_accuracy: 0.5903\n",
            "Epoch 5/20\n",
            "70/70 [==============================] - 4s 63ms/step - loss: 0.9914 - accuracy: 0.5264 - val_loss: 0.9274 - val_accuracy: 0.5921\n",
            "Epoch 6/20\n",
            "70/70 [==============================] - 6s 87ms/step - loss: 0.9872 - accuracy: 0.5263 - val_loss: 0.9090 - val_accuracy: 0.5957\n",
            "Epoch 7/20\n",
            "70/70 [==============================] - 4s 63ms/step - loss: 0.9650 - accuracy: 0.5433 - val_loss: 0.9914 - val_accuracy: 0.5072\n",
            "Epoch 8/20\n",
            "70/70 [==============================] - 4s 63ms/step - loss: 0.9596 - accuracy: 0.5578 - val_loss: 0.9073 - val_accuracy: 0.6426\n",
            "Epoch 9/20\n",
            "70/70 [==============================] - 4s 64ms/step - loss: 0.9410 - accuracy: 0.5538 - val_loss: 0.9168 - val_accuracy: 0.5884\n",
            "Epoch 10/20\n",
            "70/70 [==============================] - 4s 64ms/step - loss: 0.9372 - accuracy: 0.5555 - val_loss: 0.8939 - val_accuracy: 0.6083\n",
            "Epoch 11/20\n",
            "70/70 [==============================] - 5s 65ms/step - loss: 0.9356 - accuracy: 0.5548 - val_loss: 0.8821 - val_accuracy: 0.6318\n",
            "Epoch 12/20\n",
            "70/70 [==============================] - 4s 64ms/step - loss: 0.9247 - accuracy: 0.5678 - val_loss: 0.8846 - val_accuracy: 0.6227\n",
            "Epoch 13/20\n",
            "70/70 [==============================] - 5s 65ms/step - loss: 0.9168 - accuracy: 0.5629 - val_loss: 0.9117 - val_accuracy: 0.5704\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.8130 - accuracy: 0.6749\n",
            "bilstm\n",
            "0.8129897117614746 0.6748554706573486\n",
            "(None, 50, 120)\n",
            "call Tensor(\"Placeholder:0\", shape=(None, 50, 120), dtype=float32) <tf.Variable 'att_layer_1/att_weight:0' shape=(120, 1) dtype=float32>\n",
            "c shape Tensor(\"att_layer_1/strided_slice:0\", shape=(120,), dtype=float32)\n",
            "(None, 120)\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 50)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 50, 100)           968900    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_3 (Spatial (None, 50, 100)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 50, 120)           77280     \n",
            "_________________________________________________________________\n",
            "att_layer_1 (AttLayer)       (None, 120)               240       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 120)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 32)                3872      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 1,050,391\n",
            "Trainable params: 1,050,391\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "call Tensor(\"model_1/bidirectional_3/concat:0\", shape=(None, 50, 120), dtype=float32) <tf.Variable 'att_layer_1/att_weight:0' shape=(120, 1) dtype=float32>\n",
            "c shape Tensor(\"model_1/att_layer_1/strided_slice:0\", shape=(120,), dtype=float32)\n",
            "call Tensor(\"model_1/bidirectional_3/concat:0\", shape=(None, 50, 120), dtype=float32) <tf.Variable 'att_layer_1/att_weight:0' shape=(120, 1) dtype=float32>\n",
            "c shape Tensor(\"model_1/att_layer_1/strided_slice:0\", shape=(120,), dtype=float32)\n",
            "58/58 [==============================] - ETA: 0s - loss: 1.0787 - accuracy: 0.3874call Tensor(\"model_1/bidirectional_3/concat:0\", shape=(None, 50, 120), dtype=float32) <tf.Variable 'att_layer_1/att_weight:0' shape=(120, 1) dtype=float32>\n",
            "c shape Tensor(\"model_1/att_layer_1/strided_slice:0\", shape=(120,), dtype=float32)\n",
            "58/58 [==============================] - 11s 106ms/step - loss: 1.0782 - accuracy: 0.3880 - val_loss: 0.9960 - val_accuracy: 0.4858\n",
            "Epoch 2/6\n",
            "58/58 [==============================] - 5s 82ms/step - loss: 0.9425 - accuracy: 0.5478 - val_loss: 0.9595 - val_accuracy: 0.5372\n",
            "Epoch 3/6\n",
            "58/58 [==============================] - 5s 82ms/step - loss: 0.8260 - accuracy: 0.6145 - val_loss: 1.0826 - val_accuracy: 0.4584\n",
            "Epoch 4/6\n",
            "58/58 [==============================] - 5s 80ms/step - loss: 0.7171 - accuracy: 0.6809 - val_loss: 0.9918 - val_accuracy: 0.5011\n",
            "call Tensor(\"model_1/bidirectional_3/concat:0\", shape=(None, 50, 120), dtype=float32) <tf.Variable 'att_layer_1/att_weight:0' shape=(120, 1) dtype=float32>\n",
            "c shape Tensor(\"model_1/att_layer_1/strided_slice:0\", shape=(120,), dtype=float32)\n",
            "22/22 [==============================] - 1s 18ms/step\n",
            "(692, 3)\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.8286 - accuracy: 0.6055\n",
            "here\n",
            "0.8285945057868958 0.6054913401603699\n",
            "0.6748554913294798\n",
            "0.567823245604121\n",
            "0.5901972526339515\n",
            "f1 bilstm\n",
            "0.5774114645377669\n",
            "692\n",
            "0.6054913294797688\n",
            "0.7115724398507427\n",
            "0.6054913294797688\n",
            "f1 bilstm attention\n",
            "0.6278740315816799\n",
            "CAG\n",
            "0.6213872832369942\n",
            "0.7317797640099398\n",
            "0.6213872832369942\n",
            "0.645812704521234\n",
            "NAG\n",
            "0.6878612716763006\n",
            "0.7077307578260685\n",
            "0.7230769230769231\n",
            "0.6856529968454259\n",
            "OAG\n",
            "0.9017341040462428\n",
            "0.7305046210175097\n",
            "0.6770542741148987\n",
            "0.6988223246287762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RthotM-V544",
        "outputId": "6c473cb7-e26c-42ba-b807-0dff3c8be893"
      },
      "source": [
        "import tensorflow as tf\n",
        "wew=['bhot badhiya kaam kiya hai']\n",
        "te = [str(x).lower()  for x in wew]\n",
        "\n",
        "tokenizer.fit_on_texts(te)\n",
        "tst = tokenizer.texts_to_sequences(te)\n",
        "tst=pad_sequences(tst, maxlen= 50)\n",
        "prdct=model.predict(tst) \n",
        "print('Comment:'+ wew[0])\n",
        "if(prdct[0][0]>prdct[0][1] and prdct[0][0]>prdct[0][2]):\n",
        "  print('This comment is Covertly Aggressive (CAG)')\n",
        "\n",
        "if(prdct[0][1]>prdct[0][0] and prdct[0][1]>prdct[0][2]):\n",
        "  print('This comment is Not Aggressive (NAG)')\n",
        "\n",
        "if(prdct[0][2]>prdct[0][0] and prdct[0][2]>prdct[0][1]):\n",
        "  print('This comment is Overtly Aggressive (OAG)')\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comment:bhot badhiya kaam kiya hai\n",
            "This comment is Not Aggressive (NAG)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCdxw-xqMzAG",
        "outputId": "ff523727-7f26-467b-b01a-88a62e49f36b"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "df1=pd.read_csv('/content/drive/MyDrive/testv2.csv')\n",
        "df_target=df1['tag']\n",
        "\n",
        "tag=[]\n",
        "\n",
        "for i in df_target:\n",
        "  if(i==\"NAG\"):\n",
        "    \n",
        "    tag.append(0)\n",
        "  elif(i==\"CAG\"):\n",
        "    tag.append(1)\n",
        "  else:\n",
        "    tag.append(2)\n",
        "\n",
        "\n",
        "df_target['num']=tag\n",
        "\n",
        "print(tag)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 2, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 2, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 2, 0, 2, 0, 2, 1, 0, 1, 1, 2, 0, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 2, 1, 1, 0, 1, 0, 1, 0, 1, 2, 0, 0, 1, 2, 0, 1, 1, 1, 1, 1, 0, 2, 0, 0, 1, 0, 1, 0, 1, 2, 0, 1, 1, 0, 2, 1, 2, 0, 0, 0, 1, 1, 1, 1, 2, 0, 0, 0, 0, 1, 1, 2, 1, 0, 0, 1, 1, 0, 2, 1, 0, 1, 0, 2, 2, 0, 0, 2, 1, 2, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 2, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 1, 1, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 1, 1, 1, 0, 1, 2, 0, 1, 0, 1, 2, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 2, 1, 2, 0, 1, 1, 0, 1, 0, 2, 2, 2, 0, 1, 2, 0, 0, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0]\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-UL_vw1lriu",
        "outputId": "51da8e05-b607-47e4-c3b6-76e75c5ce503"
      },
      "source": [
        "# workbook = load_workbook(filename=\"tpmt.xlsx\")\n",
        "# sheet=workbook.active\n",
        "# l=[]\n",
        "\n",
        "# for i in range (1,len(tmpMat)+1):\n",
        "#   if(sheet.cell(row=i,column=1).value=='1'):\n",
        "#     l.append('CAG')\n",
        "#   if(sheet.cell(row=i,column=2).value=='1'):\n",
        "#     l.append('NAG')\n",
        "\n",
        "#   if(sheet.cell(row=i,column=3).value=='1'):\n",
        "#     l.append('OAG')  \n",
        "     \n",
        "# wb1 = Workbook() \n",
        "# sheet2 = wb1.add_sheet('Sheet 1')\n",
        "\n",
        "# for i in range(0,len(tmpMat)):\n",
        "#     sheet2.write(i, 0,  l[i])\n",
        "# wb1.save('predictedlables.xls')    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/openpyxl/worksheet/header_footer.py:49: UserWarning: Cannot parse header or footer so it will be ignored\n",
            "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7C2DZEz8PHh",
        "outputId": "6b751ada-6c89-452d-8802-fc87d2657d90"
      },
      "source": [
        "l=[]\n",
        "for i in range(0,len(tmpMat)):\n",
        "  if(tmpMat[i][0]==1):\n",
        "    l.append('CAG')\n",
        "  if(tmpMat[i][1]==1):\n",
        "    l.append('NAG')\n",
        "  if(tmpMat[i][2]==1):\n",
        "    l.append('OAG')\n",
        "\n",
        "print(l)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['NAG', 'CAG', 'NAG', 'CAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'CAG', 'CAG', 'OAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'OAG', 'CAG', 'CAG', 'CAG', 'CAG', 'OAG', 'CAG', 'OAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'OAG', 'NAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'OAG', 'NAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'OAG', 'CAG', 'CAG', 'OAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'OAG', 'CAG', 'CAG', 'OAG', 'CAG', 'OAG', 'CAG', 'NAG', 'CAG', 'OAG', 'OAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'OAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'OAG', 'OAG', 'CAG', 'CAG', 'NAG', 'NAG', 'OAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'OAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'OAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'OAG', 'CAG', 'CAG', 'CAG', 'CAG', 'OAG', 'CAG', 'OAG', 'CAG', 'CAG', 'CAG', 'CAG', 'OAG', 'CAG', 'OAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'OAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'OAG', 'OAG', 'OAG', 'CAG', 'NAG', 'OAG', 'CAG', 'CAG', 'OAG', 'CAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'OAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'OAG', 'CAG', 'OAG', 'CAG', 'CAG', 'OAG', 'NAG', 'OAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'NAG', 'OAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'OAG', 'NAG', 'CAG', 'OAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'OAG', 'CAG', 'NAG', 'CAG', 'OAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'CAG', 'OAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'OAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'OAG', 'CAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'NAG', 'OAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'CAG', 'OAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'CAG', 'OAG', 'NAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'CAG', 'OAG', 'CAG', 'NAG', 'CAG', 'OAG', 'CAG', 'NAG', 'CAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'CAG', 'CAG', 'NAG', 'CAG', 'NAG', 'OAG', 'NAG', 'CAG', 'CAG', 'NAG', 'OAG', 'OAG', 'CAG', 'NAG', 'CAG', 'CAG', 'CAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'NAG', 'CAG', 'NAG', 'NAG', 'OAG', 'OAG', 'NAG']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG70yw_zWnbO"
      },
      "source": [
        "#Ensemble\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "dfx=pd.DataFrame(l,columns=[\"predicted\"])\n",
        "# print(dfx['predicted'])\n",
        "df_target=df_test['tag']\n",
        "df_pred=dfx['predicted']\n",
        "print(df_target)\n",
        "print(df_pred)\n",
        "\n",
        "tag=[]\n",
        "pred=[]\n",
        "for i in df_target:\n",
        "  if(i==\"NAG\"):\n",
        "    tag.append(0)\n",
        "  elif(i==\"CAG\"):\n",
        "    tag.append(1)\n",
        "  else:\n",
        "    tag.append(2)\n",
        "\n",
        "for j in df_pred:\n",
        "  if(j==\"NAG\"):\n",
        "    k=[0,0]\n",
        "    pred.append(k)\n",
        "  elif(j==\"CAG\"):\n",
        "    k=[1,0]\n",
        "    pred.append(k)\n",
        "  else:\n",
        "    k=[2,0]\n",
        "    pred.append(k)\n",
        "print(pred)\n",
        "\n",
        "print(tag)\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQauBiI_tjM"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf1=LogisticRegression()\n",
        "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "clf3 = GaussianNB()\n",
        "\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "        ('lr',clf1),('rf', clf2), ('gnb', clf3)], voting='soft')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijvhRYy0_xML",
        "outputId": "559e39bb-435a-4827-d92d-a1a94b537c11"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "clf2.fit(pred,tag)\n",
        "clf3.fit(pred,tag)\n",
        "ensemble.fit(pred,tag)\n",
        "scores = cross_val_score(ensemble, pred, tag, scoring='accuracy',error_score='raise')\n",
        "print(mean(scores))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6690751746428945\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}